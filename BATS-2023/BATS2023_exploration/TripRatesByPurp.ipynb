{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6ea46f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked trip file copied from:\n",
      "M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\03b-assign_day\\wt-wkday_3day\\trip.csv\n",
      "to:\n",
      "M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\Linked_Trip_Analysis\\trip_linked.csv\n"
     ]
    }
   ],
   "source": [
    "# intial tabulation of linked trips from BATS 2023\n",
    "# background:\n",
    "# this analysis uses the linked trip analysis ran on 20250728\n",
    "# which uses the tue to thu weighted data from February 2025 (WeightedDataset_02212025)\n",
    "# the config for the trip linking process can be found in: https://github.com/ZephyrTransport/travel-diary-survey-tools/blob/df723a6376c0859b34e0fec89206c681b485353d/config/pipeline_config_mtc.toml\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Make a copy of the linked trip file to a new analysis folder\n",
    "TripLinking_csv = r\"M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\03b-assign_day\\wt-wkday_3day\\trip.csv\"\n",
    "TripLinking_csv_renamed = r\"M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\Linked_Trip_Analysis\\trip_linked.csv\"\n",
    "\n",
    "shutil.copy(TripLinking_csv, TripLinking_csv_renamed)\n",
    "\n",
    "print(f\"Linked trip file copied from:\\n{TripLinking_csv}\\nto:\\n{TripLinking_csv_renamed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e30d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from enum import Enum\n",
    "\n",
    "# Read the linked trip file\n",
    "LinkedTrips_filepath = r\"M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\Linked_Trip_Analysis\\trip_linked.csv\"\n",
    "LinkedTrips_df = pd.read_csv(LinkedTrips_filepath)\n",
    "\n",
    "# Define enums for mode, path and purpose\n",
    "class Mode(Enum):\n",
    "    OTHER = 0\n",
    "    WALK = 1\n",
    "    BIKE = 2\n",
    "    DA = 3\n",
    "    HOV2 = 4\n",
    "    HOV3 = 5\n",
    "    WALKTRAN = 6\n",
    "    DRIVETRAN = 7\n",
    "    SCHBUS = 8\n",
    "    TNC = 9\n",
    "\n",
    "class Path(Enum):\n",
    "    NONE = 0\n",
    "    FULLNETWORK = 1\n",
    "    NO_TOLL_NETWORK = 2 # not used in the current processing\n",
    "    BUS = 3\n",
    "    LRT = 4\n",
    "    PREMIUM = 5\n",
    "    BART = 6\n",
    "    FERRY = 7\n",
    "\n",
    "class Dpurp(Enum):\n",
    "    HOME = 0\n",
    "    WORK = 1\n",
    "    SCHOOL = 2\n",
    "    ESCORT = 3\n",
    "    PERS_BUS = 4\n",
    "    SHOP = 5\n",
    "    MEAL = 6\n",
    "    SOCREC = 7\n",
    "    CHANGE_MODE = 10 # change mode is still in the dataset, but it is only a very small number of linked trips (23 cases)\n",
    "    OTHER = 11\n",
    "    MISSING = -1\n",
    "\n",
    "\n",
    "# Map numeric codes to enum names\n",
    "LinkedTrips_df[\"mode_enum\"] = LinkedTrips_df[\"mode\"].apply(\n",
    "    lambda x: Mode(x).name if x in [m.value for m in Mode] else None\n",
    ")\n",
    "\n",
    "LinkedTrips_df[\"path_enum\"] = LinkedTrips_df[\"pathtype\"].apply(\n",
    "    lambda x: Path(x).name if x in [p.value for p in Path] else None\n",
    ")\n",
    "\n",
    "LinkedTrips_df[\"dpurp_enum\"] = LinkedTrips_df[\"dpurp\"].apply(\n",
    "    lambda x: Dpurp(x).name if x in [d.value for d in Dpurp] else None\n",
    ")\n",
    "\n",
    "# View the first few rows (but make sure no indivdiual records are shown before committing to GitHub)\n",
    "# LinkedTrips_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6bc97b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trips_byDpurp_df saved to M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\Linked_Trip_Analysis\\trips_byDpurp.csv\n"
     ]
    }
   ],
   "source": [
    "# calculated weighted number of trips by purpose\n",
    "# generate a person-day level trips by destination purpose file\n",
    "trips_byDpurp_df = (\n",
    "    LinkedTrips_df\n",
    "    .groupby(['hhno', 'pno', 'day', 'dpurp_enum'])\n",
    "    .agg(\n",
    "        trip_count=('dpurp_enum', 'size'),  # count number of rows\n",
    "        trexpfac_sum=('trexpfac', 'sum')     # sum trexpfac\n",
    "    )\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex columns created by unstack\n",
    "trips_byDpurp_df.columns = ['_'.join(map(str, col)).strip('_') for col in trips_byDpurp_df.columns.values]\n",
    "trips_byDpurp_df = trips_byDpurp_df.reset_index()\n",
    "\n",
    "output_path = r\"M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\Linked_Trip_Analysis\\trips_byDpurp.csv\"\n",
    "trips_byDpurp_df.to_csv(output_path, index=False)\n",
    "print(f\"trips_byDpurp_df saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c524c947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person_wTripCount_df saved to M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\Linked_Trip_Analysis\\person_wTripCount.csv\n"
     ]
    }
   ],
   "source": [
    "# Read person file\n",
    "person_path = r\"M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\03b-assign_day\\wt-wkday_3day\\person.csv\"\n",
    "person_df = pd.read_csv(person_path)\n",
    "\n",
    "# Join person_df with trips_byDpurp_df (left join to keep all person records) \n",
    "# note that some people participated 1 day, some people participated multiple days, but this is taken care of by the weight\n",
    "person_wTripCount_df = pd.merge(\n",
    "    person_df,\n",
    "    trips_byDpurp_df,\n",
    "    on=['hhno', 'pno'],   \n",
    "    how='left'           \n",
    ")\n",
    "\n",
    "# fill 0 for no trip\n",
    "person_wTripCount_df = person_wTripCount_df.fillna(0)\n",
    "\n",
    "output_path = r\"M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\Linked_Trip_Analysis\\person_wTripCount.csv\"\n",
    "\n",
    "person_wTripCount_df.to_csv(output_path, index=False)\n",
    "print(f\"person_wTripCount_df saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63afb29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sumTrips_by_pptyp_df saved to M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\Linked_Trip_Analysis\\sumTrips_by_pptyp.csv\n"
     ]
    }
   ],
   "source": [
    "# list of columns to sum\n",
    "cols_to_sum = [\n",
    "    'trexpfac_sum_CHANGE_MODE', 'trexpfac_sum_ESCORT', 'trexpfac_sum_HOME',\n",
    "    'trexpfac_sum_MEAL', 'trexpfac_sum_OTHER', 'trexpfac_sum_PERS_BUS',\n",
    "    'trexpfac_sum_SCHOOL', 'trexpfac_sum_SHOP', 'trexpfac_sum_SOCREC', 'trexpfac_sum_WORK'\n",
    "]\n",
    "\n",
    "# group by pptyp and sum the selected columns\n",
    "sumTrips_by_pptyp_df = (\n",
    "    person_wTripCount_df\n",
    "    .groupby('pptyp')[cols_to_sum]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "output_path = r\"M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\Linked_Trip_Analysis\\sumTrips_by_pptyp.csv\"\n",
    "\n",
    "sumTrips_by_pptyp_df.to_csv(output_path, index=False)\n",
    "print(f\"sumTrips_by_pptyp_df saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23dfbb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted persons: 15985\n",
      "Weighted persons: 7326898.264751358\n"
     ]
    }
   ],
   "source": [
    "# Read person file\n",
    "person_path = r\"M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\03b-assign_day\\wt-wkday_3day\\person.csv\"\n",
    "person_df = pd.read_csv(person_path)\n",
    "\n",
    "# Count the number of rows (unweighted count)\n",
    "num_persons_unweighted = len(person_df)\n",
    "\n",
    "# Sum the person expansion factor (weighted count)\n",
    "num_persons_weighted = person_df['psexpfac'].sum()\n",
    "\n",
    "\n",
    "print(f\"Unweighted persons: {num_persons_unweighted}\")\n",
    "print(f\"Weighted persons: {num_persons_weighted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf2bdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted persons (pptyp=1): 8319\n",
      "Weighted persons (pptyp=1): 3225037.287001975\n"
     ]
    }
   ],
   "source": [
    "# I might want to focus on pptyp=1, Full-time worker (age16+), to make the trip rates more intuitive to understand\n",
    "\n",
    "num_pptyp1_unweighted = (person_df['pptyp'] == 1).sum()\n",
    "num_pptyp1_weighted = person_df[person_df['pptyp'] == 1]['psexpfac'].sum()\n",
    "\n",
    "print(f\"Unweighted persons (pptyp=1): {num_pptyp1_unweighted}\")\n",
    "print(f\"Weighted persons (pptyp=1): {num_pptyp1_weighted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19334e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pptyp  num_persons_unweighted  num_persons_weighted\n",
      "0      1                    8319          3.225037e+06\n",
      "1      2                    1018          7.404677e+05\n",
      "2      3                    2224          9.105070e+05\n",
      "3      4                    1818          8.427431e+05\n",
      "4      5                     731          3.559589e+05\n",
      "5      6                      35          1.251005e+04\n",
      "6      7                    1260          8.891066e+05\n",
      "7      8                     580          3.505676e+05\n"
     ]
    }
   ],
   "source": [
    "# Group by pptyp and calculate num_person\n",
    "num_persons_df = person_df.groupby('pptyp').agg(\n",
    "    num_persons_unweighted=('pptyp', 'size'),\n",
    "    num_persons_weighted=('psexpfac', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "print(num_persons_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59650b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tripRates_by_pptyp_df_df saved to M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\Linked_Trip_Analysis\\tripRates_by_pptyp_df.csv\n"
     ]
    }
   ],
   "source": [
    "# join num_persons_df with sumTrips_by_pptyp_df on pptyp \n",
    "\n",
    "tripRates_by_pptyp_df = pd.merge(\n",
    "    num_persons_df,\n",
    "    sumTrips_by_pptyp_df,\n",
    "    on='pptyp',\n",
    "    how='left'   # keeps all rows from num_persons_df, adds matches from sumTrips_by_pptyp_df\n",
    ")\n",
    "\n",
    "output_path = r\"M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\Linked_Trip_Analysis\\tripRates_by_pptyp_df.csv\"\n",
    "\n",
    "tripRates_by_pptyp_df.to_csv(output_path, index=False)\n",
    "print(f\"tripRates_by_pptyp_df_df saved to {output_path}\")\n",
    "\n",
    "# the trip rates by person type seems pretty reasonable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c460b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at trip rate of telecommuters. Perhaps (work from home 3 or more days a week) vs trip rate of other workers (based on telework_freq).\n",
    "# Read the person file with  more background information\n",
    "person_background_file_path = r\"M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Full Weighted 2023 Dataset\\WeightedDataset_02212025\\person.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "person_background_df = pd.read_csv(person_background_file_path)\n",
    "\n",
    "# Keep only the desired columns and rename hh_id to hhno\n",
    "person_background_df = person_background_df[['person_id', 'hh_id', 'telework_freq']].rename(columns={'hh_id': 'hhno'})\n",
    "\n",
    "\n",
    "# Create pno as the last two digits of person_id and convert to numeric\n",
    "person_background_df['pno'] = (\n",
    "    person_background_df['person_id']\n",
    "    .astype(str)\n",
    "    .str[-2:]\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Preview the dataframe (but don't commit the preview to github)\n",
    "#person_background_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e4b23e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   telework_freq  num_persons_unweighted  num_persons_weighted\n",
      "0              1                     180          8.694760e+04\n",
      "1              2                     652          2.555789e+05\n",
      "2              3                     564          1.776140e+05\n",
      "3              4                     833          2.902242e+05\n",
      "4              5                    1212          4.150588e+05\n",
      "5              6                     601          2.166517e+05\n",
      "6              7                     514          2.465600e+05\n",
      "7              8                     427          1.932492e+05\n",
      "8            995                    8760          4.007171e+06\n",
      "9            996                    2242          1.437843e+06\n"
     ]
    }
   ],
   "source": [
    "#Join person_background_df to num_persons_df\n",
    "person_wTeleworkFreq_df = pd.merge(\n",
    "    person_df,\n",
    "    person_background_df,\n",
    "    on=['hhno', 'pno'],      \n",
    "    how='left' \n",
    ")              \n",
    "\n",
    "# Group by telework_freq and calculate num_person\n",
    "num_persons_ByTelework_df = person_wTeleworkFreq_df.groupby('telework_freq').agg(\n",
    "    num_persons_unweighted=('telework_freq', 'size'),\n",
    "    num_persons_weighted=('psexpfac', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "print(num_persons_ByTelework_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bd486984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sumTrips_by_telework_freq_df saved to M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\Linked_Trip_Analysis\\sumTrips_by_telework_freq.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#sum up the trips\n",
    "# list of columns to sum\n",
    "cols_to_sum = [\n",
    "    'trexpfac_sum_CHANGE_MODE', 'trexpfac_sum_ESCORT', 'trexpfac_sum_HOME',\n",
    "    'trexpfac_sum_MEAL', 'trexpfac_sum_OTHER', 'trexpfac_sum_PERS_BUS',\n",
    "    'trexpfac_sum_SCHOOL', 'trexpfac_sum_SHOP', 'trexpfac_sum_SOCREC', 'trexpfac_sum_WORK'\n",
    "]\n",
    "\n",
    "#Join person_background_df to person_wTripCount_df\n",
    "person_wTripCountwTeleworkFreq_df = pd.merge(\n",
    "    person_wTripCount_df,\n",
    "    person_background_df,\n",
    "    on=['hhno', 'pno'],      \n",
    "    how='left' \n",
    ")   \n",
    "\n",
    "# group by telework_freq and sum the selected columns\n",
    "sumTrips_by_telework_freq_df = (\n",
    "    person_wTripCountwTeleworkFreq_df\n",
    "    .groupby('telework_freq')[cols_to_sum]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "output_path = r\"M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\Linked_Trip_Analysis\\sumTrips_by_telework_freq.csv\"\n",
    "\n",
    "sumTrips_by_telework_freq_df.to_csv(output_path, index=False)\n",
    "print(f\"sumTrips_by_telework_freq_df saved to {output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb168a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'telework_freq'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_5232\\3639707129.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m  \u001b[38;5;66;03m#join num_persons_df with sumTrips_by_telework_freq_df on telework_freq\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m tripRates_by_teleworkFreq_df = pd.merge(\n\u001b[32m      4\u001b[39m     num_persons_df,\n\u001b[32m      5\u001b[39m     sumTrips_by_telework_freq_df,\n\u001b[32m      6\u001b[39m     on=\u001b[33m'telework_freq'\u001b[39m,\n",
      "\u001b[32mc:\\Users\\ftsang\\AppData\\Local\\anaconda3\\envs\\test\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\ftsang\\AppData\\Local\\anaconda3\\envs\\test\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32mc:\\Users\\ftsang\\AppData\\Local\\anaconda3\\envs\\test\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1307\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1308\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1309\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1310\u001b[39m                         lk = cast(Hashable, lk)\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m                         left_keys.append(left._get_label_or_level_values(lk))\n\u001b[32m   1312\u001b[39m                         join_names.append(lk)\n\u001b[32m   1313\u001b[39m                     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1314\u001b[39m                         \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\ftsang\\AppData\\Local\\anaconda3\\envs\\test\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'telework_freq'"
     ]
    }
   ],
   "source": [
    " #join num_persons_df with sumTrips_by_telework_freq_df on telework_freq\n",
    "\n",
    "tripRates_by_teleworkFreq_df = pd.merge(\n",
    "    num_persons_ByTelework_df,\n",
    "    sumTrips_by_telework_freq_df,\n",
    "    on='telework_freq',\n",
    "    how='left'   \n",
    ")\n",
    "\n",
    "output_path = r\"M:\\Data\\HomeInterview\\Bay Area Travel Study 2023\\Data\\Processed\\TripLinking_20250728\\Linked_Trip_Analysis\\tripRates_by_teleworkFreq_df.csv\"\n",
    "\n",
    "tripRates_by_teleworkFreq_df.to_csv(output_path, index=False)\n",
    "print(f\"tripRates_by_teleworkFreq_df_df saved to {output_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
